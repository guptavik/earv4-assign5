{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.23.0+cu128)\n",
            "Requirement already satisfied: numpy in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (2.2.4)\n",
            "Requirement already satisfied: torch==2.8.0+cu128 in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (2.8.0+cu128)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0+cu128->torchvision) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0+cu128->torchvision) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0+cu128->torchvision) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0+cu128->torchvision) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0+cu128->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0+cu128->torchvision) (2025.7.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch==2.8.0+cu128->torchvision) (78.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch==2.8.0+cu128->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch==2.8.0+cu128->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "0m2JWFliFfKT"
      },
      "outputs": [],
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "h_Cx9q2QFgM7"
      },
      "outputs": [],
      "source": [
        "class ImprovedNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedNet, self).__init__()\n",
        "        \n",
        "        # Convolutional Block 1 - Slightly increased channels\n",
        "        self.conv1 = nn.Conv2d(1, 10, 3, padding=1)   # 1->10 channels (was 8)\n",
        "        self.bn1 = nn.BatchNorm2d(10)\n",
        "        self.dropout1 = nn.Dropout2d(0.1)\n",
        "        \n",
        "        # Convolutional Block 2 - Slightly increased channels\n",
        "        self.conv2 = nn.Conv2d(10, 20, 3, padding=1)  # 10->20 channels (was 16)\n",
        "        self.bn2 = nn.BatchNorm2d(20)\n",
        "        self.dropout2 = nn.Dropout2d(0.1)\n",
        "        \n",
        "        # Max Pooling\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # 28x28 -> 14x14\n",
        "        \n",
        "        # Convolutional Block 3 - Increased channels\n",
        "        self.conv3 = nn.Conv2d(20, 30, 3, padding=1)  # 20->30 channels (was 16)\n",
        "        self.bn3 = nn.BatchNorm2d(30)\n",
        "        self.dropout3 = nn.Dropout2d(0.1)\n",
        "        \n",
        "        # Convolutional Block 4 - Increased channels\n",
        "        self.conv4 = nn.Conv2d(30, 40, 3, padding=1)  # 30->40 channels (was 32)\n",
        "        self.bn4 = nn.BatchNorm2d(40)\n",
        "        self.dropout4 = nn.Dropout2d(0.1)\n",
        "        \n",
        "        # Max Pooling\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  # 14x14 -> 7x7\n",
        "        \n",
        "        # Convolutional Block 5 - Increased channels\n",
        "        self.conv5 = nn.Conv2d(40, 40, 3, padding=1)  # 40->40 channels (was 32)\n",
        "        self.bn5 = nn.BatchNorm2d(40)\n",
        "        self.dropout5 = nn.Dropout2d(0.1)\n",
        "        \n",
        "        # NEW: Additional Convolutional Block 6 - Conservative\n",
        "        self.conv6 = nn.Conv2d(40, 40, 3, padding=1)  # 40->40 channels (same size)\n",
        "        self.bn6 = nn.BatchNorm2d(40)\n",
        "        self.dropout6 = nn.Dropout2d(0.1)\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)  # 7x7 -> 1x1\n",
        "        \n",
        "        # Final classification layer - Increased input size\n",
        "        self.fc = nn.Linear(40, 10)  # 40->10 (was 32->10)\n",
        "        self.dropout_fc = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = self.dropout1(F.relu(self.bn1(self.conv1(x))))\n",
        "        \n",
        "        # Block 2\n",
        "        x = self.dropout2(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        # Block 3\n",
        "        x = self.dropout3(F.relu(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        # Block 4\n",
        "        x = self.dropout4(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        # Block 5\n",
        "        x = self.dropout5(F.relu(self.bn5(self.conv5(x))))\n",
        "        \n",
        "        # NEW: Block 6 - Additional feature extraction (same channels)\n",
        "        x = self.dropout6(F.relu(self.bn6(self.conv6(x))))\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Final classification\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "xdydjYTZFyi3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.5.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.2)\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.2.4)\n",
            "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\gupta\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Using device: cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 28, 28]             100\n",
            "       BatchNorm2d-2           [-1, 10, 28, 28]              20\n",
            "         Dropout2d-3           [-1, 10, 28, 28]               0\n",
            "            Conv2d-4           [-1, 20, 28, 28]           1,820\n",
            "       BatchNorm2d-5           [-1, 20, 28, 28]              40\n",
            "         Dropout2d-6           [-1, 20, 28, 28]               0\n",
            "         MaxPool2d-7           [-1, 20, 14, 14]               0\n",
            "            Conv2d-8           [-1, 30, 14, 14]           5,430\n",
            "       BatchNorm2d-9           [-1, 30, 14, 14]              60\n",
            "        Dropout2d-10           [-1, 30, 14, 14]               0\n",
            "           Conv2d-11           [-1, 40, 14, 14]          10,840\n",
            "      BatchNorm2d-12           [-1, 40, 14, 14]              80\n",
            "        Dropout2d-13           [-1, 40, 14, 14]               0\n",
            "        MaxPool2d-14             [-1, 40, 7, 7]               0\n",
            "           Conv2d-15             [-1, 40, 7, 7]          14,440\n",
            "      BatchNorm2d-16             [-1, 40, 7, 7]              80\n",
            "        Dropout2d-17             [-1, 40, 7, 7]               0\n",
            "           Conv2d-18             [-1, 40, 7, 7]          14,440\n",
            "      BatchNorm2d-19             [-1, 40, 7, 7]              80\n",
            "        Dropout2d-20             [-1, 40, 7, 7]               0\n",
            "AdaptiveAvgPool2d-21             [-1, 40, 1, 1]               0\n",
            "          Dropout-22                   [-1, 40]               0\n",
            "           Linear-23                   [-1, 10]             410\n",
            "================================================================\n",
            "Total params: 47,840\n",
            "Trainable params: 47,840\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.99\n",
            "Params size (MB): 0.18\n",
            "Estimated Total Size (MB): 1.17\n",
            "----------------------------------------------------------------\n",
            "\n",
            "Total parameters: 47,840\n",
            "Trainable parameters: 47,840\n",
            "Parameter count < 20k: False\n",
            "\n",
            "=== DETAILED PARAMETER BREAKDOWN ===\n",
            "Conv1 (1‚Üí10): 90 parameters\n",
            "Conv2 (10‚Üí20): 1,800 parameters\n",
            "Conv3 (20‚Üí30): 5,400 parameters\n",
            "Conv4 (30‚Üí40): 10,800 parameters\n",
            "Conv5 (40‚Üí40): 14,400 parameters\n",
            "Conv6 (40‚Üí40): 14,400 parameters\n",
            "BatchNorm layers: ~360 parameters\n",
            "FC layer (40‚Üí10): 410 parameters\n",
            "Total calculated: 47,660 parameters\n"
          ]
        }
      ],
      "source": [
        "%pip install torchsummary scikit-learn\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create and test the improved model\n",
        "model = ImprovedNet().to(device)\n",
        "summary(model, input_size=(1, 28, 28))\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Parameter count < 20k: {total_params < 20000}\")\n",
        "\n",
        "# Detailed parameter breakdown\n",
        "print(f\"\\n=== DETAILED PARAMETER BREAKDOWN ===\")\n",
        "print(f\"Conv1 (1‚Üí10): {1*3*3*10:,} parameters\")\n",
        "print(f\"Conv2 (10‚Üí20): {10*3*3*20:,} parameters\")\n",
        "print(f\"Conv3 (20‚Üí30): {20*3*3*30:,} parameters\")\n",
        "print(f\"Conv4 (30‚Üí40): {30*3*3*40:,} parameters\")\n",
        "print(f\"Conv5 (40‚Üí40): {40*3*3*40:,} parameters\")\n",
        "print(f\"Conv6 (40‚Üí40): {40*3*3*40:,} parameters\")\n",
        "print(f\"BatchNorm layers: ~{10*2 + 20*2 + 30*2 + 40*2 + 40*2 + 40*2:,} parameters\")\n",
        "print(f\"FC layer (40‚Üí10): {40*10 + 10:,} parameters\")\n",
        "print(f\"Total calculated: {1*3*3*10 + 10*3*3*20 + 20*3*3*30 + 30*3*3*40 + 40*3*3*40 + 40*3*3*40 + (10*2 + 20*2 + 30*2 + 40*2 + 40*2 + 40*2) + (40*10 + 10):,} parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "DqTWLaM5GHgH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TRAINING OPTIMIZATIONS ===\n",
            "Training samples: 50000 (with data augmentation)\n",
            "Validation samples: 10000\n",
            "Test samples: 10000\n",
            "Data augmentation: RandomRotation(10¬∞), RandomAffine(translate=0.1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "# üîß TRAINING OPTIMIZATIONS - Enhanced Data Augmentation\n",
        "print(\"=== TRAINING OPTIMIZATIONS ===\")\n",
        "\n",
        "# Enhanced transforms for training with data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomRotation(10),                    # ¬±10 degrees rotation\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Translation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Standard transforms for validation and test (no augmentation)\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Load MNIST dataset\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "# Load full training dataset with augmented transforms\n",
        "full_train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform_train)\n",
        "\n",
        "# Create train/validation split (50k train, 10k validation)\n",
        "train_size = 50000\n",
        "val_size = 10000\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    full_train_dataset, [train_size, val_size], \n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "# Test dataset (10k samples) with standard transforms\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transform_test),\n",
        "    batch_size=batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)} (with data augmentation)\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_loader.dataset)}\")\n",
        "print(f\"Data augmentation: RandomRotation(10¬∞), RandomAffine(translate=0.1)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8fDefDhaFlwH"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        \n",
        "        pbar.set_description(f'Epoch {epoch} - Loss: {loss.item():.4f}')\n",
        "    \n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = 100. * correct / len(train_loader.dataset)\n",
        "    \n",
        "    return train_loss, train_acc\n",
        "\n",
        "def validate(model, device, val_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            val_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_acc = 100. * correct / len(val_loader.dataset)\n",
        "    \n",
        "    return val_loss, val_acc\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = 100. * correct / len(test_loader.dataset)\n",
        "    \n",
        "    print(f'\\nTest Results:')\n",
        "    print(f'Average loss: {test_loss:.4f}')\n",
        "    print(f'Accuracy: {correct}/{len(test_loader.dataset)} ({test_acc:.2f}%)')\n",
        "    \n",
        "    return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MMWbLWO6FuHb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Loss: 0.8853: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 45.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1: Train Loss: 0.0120, Train Acc: 49.80% | Val Loss: 0.7305, Val Acc: 88.49%\n",
            "  ‚Üí New best validation accuracy: 88.49%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Loss: 0.4419: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 47.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  2: Train Loss: 0.0053, Train Acc: 81.15% | Val Loss: 0.2754, Val Acc: 94.90%\n",
            "  ‚Üí New best validation accuracy: 94.90%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Loss: 0.4326: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  3: Train Loss: 0.0033, Train Acc: 88.38% | Val Loss: 0.1767, Val Acc: 96.20%\n",
            "  ‚Üí New best validation accuracy: 96.20%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Loss: 0.1873: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  4: Train Loss: 0.0025, Train Acc: 91.04% | Val Loss: 0.1270, Val Acc: 96.90%\n",
            "  ‚Üí New best validation accuracy: 96.90%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Loss: 0.3159: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 47.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  5: Train Loss: 0.0022, Train Acc: 91.86% | Val Loss: 0.1104, Val Acc: 97.02%\n",
            "  ‚Üí New best validation accuracy: 97.02%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6 - Loss: 0.3396: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  6: Train Loss: 0.0020, Train Acc: 92.71% | Val Loss: 0.1014, Val Acc: 97.16%\n",
            "  ‚Üí New best validation accuracy: 97.16%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7 - Loss: 0.1583: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  7: Train Loss: 0.0019, Train Acc: 93.13% | Val Loss: 0.0992, Val Acc: 97.20%\n",
            "  ‚Üí New best validation accuracy: 97.20%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8 - Loss: 0.1877: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  8: Train Loss: 0.0017, Train Acc: 93.92% | Val Loss: 0.0857, Val Acc: 97.58%\n",
            "  ‚Üí New best validation accuracy: 97.58%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9 - Loss: 0.2036: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  9: Train Loss: 0.0016, Train Acc: 94.23% | Val Loss: 0.0824, Val Acc: 97.68%\n",
            "  ‚Üí New best validation accuracy: 97.68%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10 - Loss: 0.1773: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Train Loss: 0.0016, Train Acc: 94.34% | Val Loss: 0.0835, Val Acc: 97.74%\n",
            "  ‚Üí New best validation accuracy: 97.74%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11 - Loss: 0.2255: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Train Loss: 0.0016, Train Acc: 94.19% | Val Loss: 0.0834, Val Acc: 97.63%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12 - Loss: 0.1184: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Train Loss: 0.0016, Train Acc: 94.45% | Val Loss: 0.0802, Val Acc: 97.71%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13 - Loss: 0.1677: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Train Loss: 0.0015, Train Acc: 94.47% | Val Loss: 0.0791, Val Acc: 97.74%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14 - Loss: 0.2315: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Train Loss: 0.0015, Train Acc: 94.56% | Val Loss: 0.0800, Val Acc: 97.61%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15 - Loss: 0.3210: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Train Loss: 0.0015, Train Acc: 94.61% | Val Loss: 0.0819, Val Acc: 97.73%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16 - Loss: 0.1303: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Train Loss: 0.0015, Train Acc: 94.60% | Val Loss: 0.0786, Val Acc: 97.81%\n",
            "  ‚Üí New best validation accuracy: 97.81%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17 - Loss: 0.1687: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Train Loss: 0.0015, Train Acc: 94.75% | Val Loss: 0.0763, Val Acc: 97.83%\n",
            "  ‚Üí New best validation accuracy: 97.83%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18 - Loss: 0.1353: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 45.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Train Loss: 0.0015, Train Acc: 94.65% | Val Loss: 0.0794, Val Acc: 97.76%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19 - Loss: 0.0841: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Train Loss: 0.0015, Train Acc: 94.80% | Val Loss: 0.0782, Val Acc: 97.63%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20 - Loss: 0.3151: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:08<00:00, 46.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Train Loss: 0.0015, Train Acc: 94.71% | Val Loss: 0.0760, Val Acc: 97.99%\n",
            "  ‚Üí New best validation accuracy: 97.99%\n",
            "==================================================\n",
            "Training completed!\n",
            "Best validation accuracy: 97.99%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Initialize model and optimizer\n",
        "model = OptimizedNet().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# Training configuration  \n",
        "epochs = 20\n",
        "best_val_acc = 0\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # Training\n",
        "    train_loss, train_acc = train(model, device, train_loader, optimizer, epoch)\n",
        "    \n",
        "    # Validation\n",
        "    val_loss, val_acc = validate(model, device, val_loader)\n",
        "    \n",
        "    # Learning rate scheduling\n",
        "    scheduler.step()\n",
        "    \n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f'Epoch {epoch:2d}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f'  ‚Üí New best validation accuracy: {val_acc:.2f}%')\n",
        "    \n",
        "    # Early stopping if target achieved\n",
        "    if val_acc >= 99.4:\n",
        "        print(f'  ‚Üí Target accuracy of 99.4% achieved!')\n",
        "        break\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(f\"Training completed!\")\n",
        "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So5uk4EkHW6R"
      },
      "outputs": [],
      "source": [
        "# Load best model and test on test set\n",
        "print(\"Loading best model and testing on test set...\")\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "test_loss, test_acc = test(model, device, test_loader)\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss', color='blue')\n",
        "plt.plot(val_losses, label='Validation Loss', color='red')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Train Accuracy', color='blue')\n",
        "plt.plot(val_accs, label='Validation Accuracy', color='red')\n",
        "plt.axhline(y=99.4, color='green', linestyle='--', label='Target (99.4%)')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model Architecture: OptimizedNet with BatchNorm, Dropout, and GAP\")\n",
        "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Parameter Count < 20k: {sum(p.numel() for p in model.parameters()) < 20000}\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n",
        "print(f\"Target Achieved (‚â•99.4%): {'‚úÖ YES' if test_acc >= 99.4 else '‚ùå NO'}\")\n",
        "print(f\"Training Epochs Used: {len(train_losses)}\")\n",
        "print(f\"Epochs < 20: {'‚úÖ YES' if len(train_losses) <= 20 else '‚ùå NO'}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Architecture Improvements - Enhanced Model\n",
        "\n",
        "## üèóÔ∏è **Improved Architecture Changes**\n",
        "\n",
        "### **Key Improvements Made:**\n",
        "\n",
        "1. **Increased Channel Progression:**\n",
        "   - Conv1: 1‚Üí10 channels (was 1‚Üí8)\n",
        "   - Conv2: 10‚Üí20 channels (was 8‚Üí16)\n",
        "   - Conv3: 20‚Üí30 channels (was 16‚Üí16)\n",
        "   - Conv4: 30‚Üí40 channels (was 16‚Üí32)\n",
        "   - Conv5: 40‚Üí40 channels (was 32‚Üí32)\n",
        "   - **NEW Conv6: 40‚Üí50 channels**\n",
        "\n",
        "2. **Additional Convolutional Layer:**\n",
        "   - Added Conv6 before Global Average Pooling\n",
        "   - Provides more feature extraction capability\n",
        "   - Increases model depth for better representation learning\n",
        "\n",
        "3. **Enhanced Final Layer:**\n",
        "   - FC layer: 50‚Üí10 (was 32‚Üí10)\n",
        "   - More features fed into classification layer\n",
        "   - Better decision-making capability\n",
        "\n",
        "### **Expected Benefits:**\n",
        "- **Better Feature Extraction**: More channels capture richer features\n",
        "- **Deeper Network**: Additional conv layer improves representation learning\n",
        "- **Enhanced Classification**: Larger FC layer with more input features\n",
        "- **Maintained Efficiency**: Still under 20k parameters\n",
        "\n",
        "### **Architecture Flow:**\n",
        "```\n",
        "Input (28√ó28√ó1)\n",
        "‚îú‚îÄ‚îÄ Conv1: 1‚Üí10 channels, 3√ó3, padding=1 ‚Üí 28√ó28√ó10\n",
        "‚îú‚îÄ‚îÄ BN1 + ReLU + Dropout2D(0.1)\n",
        "‚îú‚îÄ‚îÄ Conv2: 10‚Üí20 channels, 3√ó3, padding=1 ‚Üí 28√ó28√ó20\n",
        "‚îú‚îÄ‚îÄ BN2 + ReLU + Dropout2D(0.1)\n",
        "‚îú‚îÄ‚îÄ MaxPool2D(2√ó2) ‚Üí 14√ó14√ó20\n",
        "‚îú‚îÄ‚îÄ Conv3: 20‚Üí30 channels, 3√ó3, padding=1 ‚Üí 14√ó14√ó30\n",
        "‚îú‚îÄ‚îÄ BN3 + ReLU + Dropout2D(0.1)\n",
        "‚îú‚îÄ‚îÄ Conv4: 30‚Üí40 channels, 3√ó3, padding=1 ‚Üí 14√ó14√ó40\n",
        "‚îú‚îÄ‚îÄ BN4 + ReLU + Dropout2D(0.1)\n",
        "‚îú‚îÄ‚îÄ MaxPool2D(2√ó2) ‚Üí 7√ó7√ó40\n",
        "‚îú‚îÄ‚îÄ Conv5: 40‚Üí40 channels, 3√ó3, padding=1 ‚Üí 7√ó7√ó40\n",
        "‚îú‚îÄ‚îÄ BN5 + ReLU + Dropout2D(0.1)\n",
        "‚îú‚îÄ‚îÄ Conv6: 40‚Üí50 channels, 3√ó3, padding=1 ‚Üí 7√ó7√ó50  [NEW]\n",
        "‚îú‚îÄ‚îÄ BN6 + ReLU + Dropout2D(0.1)                      [NEW]\n",
        "‚îú‚îÄ‚îÄ Global Average Pooling ‚Üí 1√ó1√ó50\n",
        "‚îú‚îÄ‚îÄ Dropout(0.2) ‚Üí FC(50‚Üí10) ‚Üí LogSoftmax\n",
        "‚îî‚îÄ‚îÄ Prediction (10 classes)\n",
        "```\n",
        "\n",
        "### **Final Conservative Architecture:**\n",
        "- **Conv1**: 1√ó3√ó3√ó10 = 90 parameters\n",
        "- **Conv2**: 10√ó3√ó3√ó20 = 1,800 parameters\n",
        "- **Conv3**: 20√ó3√ó3√ó30 = 5,400 parameters\n",
        "- **Conv4**: 30√ó3√ó3√ó40 = 10,800 parameters\n",
        "- **Conv5**: 40√ó3√ó3√ó40 = 14,400 parameters\n",
        "- **Conv6**: 40√ó3√ó3√ó40 = 14,400 parameters (NEW - same channels)\n",
        "- **BatchNorm**: ~240 parameters\n",
        "- **FC Layer**: 40√ó10 + 10 = 410 parameters\n",
        "- **Total**: ~47,500 parameters\n",
        "\n",
        "**Note**: This still exceeds 20k parameters. Let's try a different approach - reduce channels but add depth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's create a more conservative architecture that stays under 20k parameters\n",
        "class ConservativeImprovedNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConservativeImprovedNet, self).__init__()\n",
        "        \n",
        "        # Convolutional Block 1 - Slightly increased channels\n",
        "        self.conv1 = nn.Conv2d(1, 10, 3, padding=1)   # 1->10 channels (was 8)\n",
        "        self.bn1 = nn.BatchNorm2d(10)\n",
        "        self.dropout1 = nn.Dropout2d(0.1)\n",
        "        \n",
        "        # Convolutional Block 2 - Slightly increased channels\n",
        "        self.conv2 = nn.Conv2d(10, 20, 3, padding=1)  # 10->20 channels (was 16)\n",
        "        self.bn2 = nn.BatchNorm2d(20)\n",
        "        self.dropout2 = nn.Dropout2d(0.1)\n",
        "        \n",
        "        # Max Pooling\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # 28x28 -> 14x14\n",
        "        \n",
        "        # Convolutional Block 3 - Increased channels\n",
        "        self.conv3 = nn.Conv2d(20, 30, 3, padding=1)  # 20->30 channels (was 16)\n",
        "        self.bn3 = nn.BatchNorm2d(30)\n",
        "        self.dropout3 = nn.Dropout2d(0.1)\n",
        "        \n",
        "        # Convolutional Block 4 - Increased channels\n",
        "        self.conv4 = nn.Conv2d(30, 40, 3, padding=1)  # 30->40 channels (was 32)\n",
        "        self.bn4 = nn.BatchNorm2d(40)\n",
        "        self.dropout4 = nn.Dropout2d(0.1)\n",
        "        \n",
        "        # Max Pooling\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  # 14x14 -> 7x7\n",
        "        \n",
        "        # Convolutional Block 5 - Increased channels\n",
        "        self.conv5 = nn.Conv2d(40, 40, 3, padding=1)  # 40->40 channels (was 32)\n",
        "        self.bn5 = nn.BatchNorm2d(40)\n",
        "        self.dropout5 = nn.Dropout2d(0.1)\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)  # 7x7 -> 1x1\n",
        "        \n",
        "        # Final classification layer - Increased input size\n",
        "        self.fc = nn.Linear(40, 10)  # 40->10 (was 32->10)\n",
        "        self.dropout_fc = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Block 1\n",
        "        x = self.dropout1(F.relu(self.bn1(self.conv1(x))))\n",
        "        \n",
        "        # Block 2\n",
        "        x = self.dropout2(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool1(x)\n",
        "        \n",
        "        # Block 3\n",
        "        x = self.dropout3(F.relu(self.bn3(self.conv3(x))))\n",
        "        \n",
        "        # Block 4\n",
        "        x = self.dropout4(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = self.pool2(x)\n",
        "        \n",
        "        # Block 5\n",
        "        x = self.dropout5(F.relu(self.bn5(self.conv5(x))))\n",
        "        \n",
        "        # Global Average Pooling\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # Final classification\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Test the conservative architecture\n",
        "print(\"=== CONSERVATIVE IMPROVED ARCHITECTURE ===\")\n",
        "conservative_model = ConservativeImprovedNet().to(device)\n",
        "summary(conservative_model, input_size=(1, 28, 28))\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in conservative_model.parameters())\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "print(f\"Parameter count < 20k: {total_params < 20000}\")\n",
        "\n",
        "# Detailed parameter breakdown\n",
        "print(f\"\\n=== DETAILED PARAMETER BREAKDOWN ===\")\n",
        "print(f\"Conv1 (1‚Üí10): {1*3*3*10:,} parameters\")\n",
        "print(f\"Conv2 (10‚Üí20): {10*3*3*20:,} parameters\")\n",
        "print(f\"Conv3 (20‚Üí30): {20*3*3*30:,} parameters\")\n",
        "print(f\"Conv4 (30‚Üí40): {30*3*3*40:,} parameters\")\n",
        "print(f\"Conv5 (40‚Üí40): {40*3*3*40:,} parameters\")\n",
        "print(f\"BatchNorm layers: ~{10*2 + 20*2 + 30*2 + 40*2 + 40*2:,} parameters\")\n",
        "print(f\"FC layer (40‚Üí10): {40*10 + 10:,} parameters\")\n",
        "print(f\"Total calculated: {1*3*3*10 + 10*3*3*20 + 20*3*3*30 + 30*3*3*40 + 40*3*3*40 + (10*2 + 20*2 + 30*2 + 40*2 + 40*2) + (40*10 + 10):,} parameters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß TRAINING OPTIMIZATIONS - Enhanced Training Setup\n",
        "\n",
        "# Initialize improved model with enhanced optimizer settings\n",
        "model = ImprovedNet().to(device)\n",
        "\n",
        "# üîß ENHANCED OPTIMIZER SETTINGS\n",
        "print(\"=== ENHANCED OPTIMIZER SETTINGS ===\")\n",
        "\n",
        "# Option 1: AdamW with better weight decay (recommended)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
        "\n",
        "# Option 2: SGD with momentum (alternative - uncomment to use)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "# üîß IMPROVED LEARNING RATE SCHEDULING\n",
        "print(\"=== ENHANCED LEARNING RATE SCHEDULING ===\")\n",
        "\n",
        "# Option 1: ReduceLROnPlateau (monitors validation accuracy)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', factor=0.5, patience=3, verbose=True, min_lr=1e-6\n",
        ")\n",
        "\n",
        "# Option 2: CosineAnnealingWarmRestarts (alternative - uncomment to use)\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "#     optimizer, T_0=5, T_mult=2, eta_min=1e-6\n",
        "# )\n",
        "\n",
        "# Option 3: StepLR (original - uncomment to use)\n",
        "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "print(f\"Optimizer: {type(optimizer).__name__}\")\n",
        "print(f\"Scheduler: {type(scheduler).__name__}\")\n",
        "print(f\"Initial LR: {optimizer.param_groups[0]['lr']}\")\n",
        "print(f\"Weight Decay: {optimizer.param_groups[0]['weight_decay']}\")\n",
        "\n",
        "# Training configuration\n",
        "epochs = 20\n",
        "best_val_acc = 0\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "val_losses = []\n",
        "val_accs = []\n",
        "\n",
        "print(f\"\\n=== TRAINING CONFIGURATION ===\")\n",
        "print(f\"Epochs: {epochs}\")\n",
        "print(f\"Batch Size: {batch_size}\")\n",
        "print(f\"Data Augmentation: Enabled\")\n",
        "print(f\"Early Stopping: Enabled (target: 99.4%)\")\n",
        "print(f\"Model Checkpointing: Enabled\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß ENHANCED TRAINING LOOP with Optimizations\n",
        "\n",
        "print(\"Starting enhanced training with optimizations...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # Training\n",
        "    train_loss, train_acc = train(model, device, train_loader, optimizer, epoch)\n",
        "    \n",
        "    # Validation\n",
        "    val_loss, val_acc = validate(model, device, val_loader)\n",
        "    \n",
        "    # üîß ENHANCED LEARNING RATE SCHEDULING\n",
        "    # For ReduceLROnPlateau, we pass validation accuracy\n",
        "    if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
        "        scheduler.step(val_acc)\n",
        "    else:\n",
        "        scheduler.step()\n",
        "    \n",
        "    # Store metrics\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "    \n",
        "    # Print epoch results with current learning rate\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f'Epoch {epoch:2d}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | LR: {current_lr:.6f}')\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'best_improved_model.pth')\n",
        "        print(f'  ‚Üí New best validation accuracy: {val_acc:.2f}%')\n",
        "    \n",
        "    # Early stopping if target achieved\n",
        "    if val_acc >= 99.4:\n",
        "        print(f'  ‚Üí Target accuracy of 99.4% achieved!')\n",
        "        break\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"Enhanced training completed!\")\n",
        "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"Final learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üîß TESTING IMPROVED MODEL with Enhanced Results\n",
        "\n",
        "# Load best improved model and test on test set\n",
        "print(\"Loading best improved model and testing on test set...\")\n",
        "model.load_state_dict(torch.load('best_improved_model.pth'))\n",
        "test_loss, test_acc = test(model, device, test_loader)\n",
        "\n",
        "# Plot enhanced training curves\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
        "plt.plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
        "plt.title('Enhanced Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs, label='Train Accuracy', color='blue', linewidth=2)\n",
        "plt.plot(val_accs, label='Validation Accuracy', color='red', linewidth=2)\n",
        "plt.axhline(y=99.4, color='green', linestyle='--', linewidth=2, label='Target (99.4%)')\n",
        "plt.title('Enhanced Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Enhanced final summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ENHANCED MODEL RESULTS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Model Architecture: ImprovedNet with Enhanced Training\")\n",
        "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Parameter Count < 20k: {sum(p.numel() for p in model.parameters()) < 20000}\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n",
        "print(f\"Target Achieved (‚â•99.4%): {'‚úÖ YES' if test_acc >= 99.4 else '‚ùå NO'}\")\n",
        "print(f\"Training Epochs Used: {len(train_losses)}\")\n",
        "print(f\"Epochs < 20: {'‚úÖ YES' if len(train_losses) <= 20 else '‚ùå NO'}\")\n",
        "print(f\"Final Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "print(f\"Optimizer Used: {type(optimizer).__name__}\")\n",
        "print(f\"Scheduler Used: {type(scheduler).__name__}\")\n",
        "print(f\"Data Augmentation: RandomRotation + RandomAffine\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîß Training Optimizations - Summary\n",
        "\n",
        "## **Enhanced Training Optimizations Implemented:**\n",
        "\n",
        "### **1. üéØ Data Augmentation**\n",
        "- **RandomRotation(10¬∞)**: Adds rotation invariance\n",
        "- **RandomAffine(translate=0.1)**: Adds translation invariance\n",
        "- **Applied only to training data**: Validation/test use standard transforms\n",
        "- **Benefits**: Better generalization, reduced overfitting\n",
        "\n",
        "### **2. üöÄ Optimizer Improvements**\n",
        "- **AdamW**: Better weight decay handling than Adam\n",
        "- **Weight Decay**: Increased to 1e-3 for stronger regularization\n",
        "- **Alternative Options**: SGD with momentum available\n",
        "- **Benefits**: More stable training, better convergence\n",
        "\n",
        "### **3. üìà Learning Rate Scheduling**\n",
        "- **ReduceLROnPlateau**: Monitors validation accuracy\n",
        "- **Factor**: 0.5 (reduces LR by half when plateau detected)\n",
        "- **Patience**: 3 epochs before reducing LR\n",
        "- **Min LR**: 1e-6 (prevents LR from becoming too small)\n",
        "- **Benefits**: Adaptive learning rate, better fine-tuning\n",
        "\n",
        "### **4. üîÑ Enhanced Training Loop**\n",
        "- **Learning Rate Monitoring**: Shows current LR in each epoch\n",
        "- **Adaptive Scheduling**: Different behavior for different schedulers\n",
        "- **Better Checkpointing**: Saves best improved model\n",
        "- **Enhanced Logging**: More detailed progress tracking\n",
        "\n",
        "### **5. üìä Improved Visualization**\n",
        "- **Enhanced Plots**: Better styling and formatting\n",
        "- **Learning Rate Tracking**: Shows LR changes over time\n",
        "- **Comprehensive Summary**: Detailed results comparison\n",
        "- **Performance Metrics**: All key metrics displayed\n",
        "\n",
        "## **Expected Improvements:**\n",
        "- **Better Generalization**: Data augmentation reduces overfitting\n",
        "- **Faster Convergence**: AdamW with better weight decay\n",
        "- **Adaptive Learning**: ReduceLROnPlateau fine-tunes automatically\n",
        "- **Higher Accuracy**: Combined optimizations should improve performance\n",
        "- **More Stable Training**: Better regularization and scheduling\n",
        "\n",
        "## **Comparison with Original:**\n",
        "| Aspect | Original | Enhanced |\n",
        "|--------|----------|----------|\n",
        "| Data Augmentation | None | RandomRotation + RandomAffine |\n",
        "| Optimizer | Adam | AdamW |\n",
        "| Weight Decay | 1e-4 | 1e-3 |\n",
        "| Scheduler | StepLR | ReduceLROnPlateau |\n",
        "| LR Monitoring | No | Yes |\n",
        "| Expected Accuracy | 98.36% | 99.0%+ |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ ADVANCED TECHNIQUES - Label Smoothing Implementation\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"\n",
        "    Label Smoothing Cross Entropy Loss\n",
        "    Reduces overfitting by preventing overconfident predictions\n",
        "    \"\"\"\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1.0 - smoothing\n",
        "    \n",
        "    def forward(self, x, target):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: model predictions (logits)\n",
        "            target: true labels\n",
        "        \"\"\"\n",
        "        logprobs = F.log_softmax(x, dim=-1)\n",
        "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
        "        nll_loss = nll_loss.squeeze(1)\n",
        "        smooth_loss = -logprobs.mean(dim=-1)\n",
        "        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n",
        "        return loss.mean()\n",
        "\n",
        "# Test label smoothing\n",
        "print(\"=== LABEL SMOOTHING IMPLEMENTATION ===\")\n",
        "criterion_smooth = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
        "print(f\"Label Smoothing: {criterion_smooth.smoothing}\")\n",
        "print(f\"Confidence: {criterion_smooth.confidence}\")\n",
        "\n",
        "# Create a simple test\n",
        "test_logits = torch.randn(4, 10)  # batch_size=4, num_classes=10\n",
        "test_targets = torch.tensor([0, 1, 2, 3])\n",
        "loss_smooth = criterion_smooth(test_logits, test_targets)\n",
        "loss_standard = F.cross_entropy(test_logits, test_targets)\n",
        "\n",
        "print(f\"Standard CrossEntropy Loss: {loss_standard:.4f}\")\n",
        "print(f\"Label Smoothing Loss: {loss_smooth:.4f}\")\n",
        "print(f\"Difference: {abs(loss_smooth - loss_standard):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ ADVANCED TECHNIQUES - Mixup Data Augmentation\n",
        "\n",
        "def mixup_data(x, y, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Mixup data augmentation\n",
        "    Creates virtual training examples by mixing pairs of examples\n",
        "    \"\"\"\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    \n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    \n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    \"\"\"\n",
        "    Mixup loss function\n",
        "    Combines losses from both original and mixed examples\n",
        "    \"\"\"\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Test mixup\n",
        "print(\"=== MIXUP DATA AUGMENTATION ===\")\n",
        "print(\"Mixup creates virtual training examples by mixing pairs of examples\")\n",
        "print(\"Benefits: Better generalization, reduced overfitting, improved robustness\")\n",
        "\n",
        "# Create test data\n",
        "test_x = torch.randn(4, 1, 28, 28)  # batch of images\n",
        "test_y = torch.tensor([0, 1, 2, 3])  # batch of labels\n",
        "\n",
        "# Apply mixup\n",
        "mixed_x, y_a, y_b, lam = mixup_data(test_x, test_y, alpha=1.0)\n",
        "print(f\"Original batch size: {test_x.shape[0]}\")\n",
        "print(f\"Mixed batch size: {mixed_x.shape[0]}\")\n",
        "print(f\"Mixing coefficient (Œª): {lam:.4f}\")\n",
        "print(f\"Original labels: {test_y.tolist()}\")\n",
        "print(f\"Mixed labels A: {y_a.tolist()}\")\n",
        "print(f\"Mixed labels B: {y_b.tolist()}\")\n",
        "\n",
        "# Test mixup criterion\n",
        "test_pred = torch.randn(4, 10)\n",
        "loss_a = F.cross_entropy(test_pred, y_a)\n",
        "loss_b = F.cross_entropy(test_pred, y_b)\n",
        "mixup_loss = mixup_criterion(F.cross_entropy, test_pred, y_a, y_b, lam)\n",
        "\n",
        "print(f\"Loss A: {loss_a:.4f}\")\n",
        "print(f\"Loss B: {loss_b:.4f}\")\n",
        "print(f\"Mixup Loss: {mixup_loss:.4f}\")\n",
        "print(f\"Expected: {lam * loss_a + (1 - lam) * loss_b:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ ADVANCED TECHNIQUES - Enhanced Training Functions\n",
        "\n",
        "def train_advanced(model, device, train_loader, optimizer, epoch, use_mixup=True, mixup_alpha=1.0):\n",
        "    \"\"\"\n",
        "    Enhanced training function with advanced techniques\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    pbar = tqdm(train_loader, desc=f'Advanced Epoch {epoch}')\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Apply mixup if enabled\n",
        "        if use_mixup and np.random.random() < 0.5:  # 50% chance to apply mixup\n",
        "            mixed_data, y_a, y_b, lam = mixup_data(data, target, alpha=mixup_alpha)\n",
        "            output = model(mixed_data)\n",
        "            \n",
        "            # Use label smoothing with mixup\n",
        "            criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
        "            loss = mixup_criterion(criterion, output, y_a, y_b, lam)\n",
        "            \n",
        "            # Calculate accuracy (approximate)\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += (lam * pred.eq(y_a.view_as(pred)).sum().item() + \n",
        "                       (1 - lam) * pred.eq(y_b.view_as(pred)).sum().item())\n",
        "        else:\n",
        "            # Standard training without mixup\n",
        "            output = model(data)\n",
        "            criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        pbar.set_description(f'Advanced Epoch {epoch} - Loss: {loss.item():.4f}')\n",
        "    \n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = 100. * correct / len(train_loader.dataset)\n",
        "    \n",
        "    return train_loss, train_acc\n",
        "\n",
        "def validate_advanced(model, device, val_loader):\n",
        "    \"\"\"\n",
        "    Enhanced validation function\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            \n",
        "            # Use label smoothing for validation too\n",
        "            criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
        "            val_loss += criterion(output, target).item()\n",
        "            \n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_acc = 100. * correct / len(val_loader.dataset)\n",
        "    \n",
        "    return val_loss, val_acc\n",
        "\n",
        "print(\"=== ADVANCED TRAINING FUNCTIONS ===\")\n",
        "print(\"Enhanced training with:\")\n",
        "print(\"‚úÖ Label Smoothing (smoothing=0.1)\")\n",
        "print(\"‚úÖ Mixup Data Augmentation (50% probability)\")\n",
        "print(\"‚úÖ Advanced Loss Functions\")\n",
        "print(\"‚úÖ Better Generalization\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ ADVANCED TECHNIQUES - Complete Training Setup\n",
        "\n",
        "# Initialize model with advanced techniques\n",
        "model_advanced = ImprovedNet().to(device)\n",
        "\n",
        "# Enhanced optimizer with advanced techniques\n",
        "optimizer_advanced = optim.AdamW(model_advanced.parameters(), lr=0.0008, weight_decay=1e-3)\n",
        "\n",
        "# Advanced learning rate scheduling\n",
        "scheduler_advanced = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer_advanced, mode='max', factor=0.5, patience=2, verbose=True, min_lr=1e-6\n",
        ")\n",
        "\n",
        "# Training configuration for advanced techniques\n",
        "epochs_advanced = 20\n",
        "best_val_acc_advanced = 0\n",
        "train_losses_advanced = []\n",
        "train_accs_advanced = []\n",
        "val_losses_advanced = []\n",
        "val_accs_advanced = []\n",
        "\n",
        "print(\"=== ADVANCED TECHNIQUES TRAINING SETUP ===\")\n",
        "print(f\"Model: ImprovedNet with Advanced Techniques\")\n",
        "print(f\"Optimizer: {type(optimizer_advanced).__name__}\")\n",
        "print(f\"Scheduler: {type(scheduler_advanced).__name__}\")\n",
        "print(f\"Initial LR: {optimizer_advanced.param_groups[0]['lr']}\")\n",
        "print(f\"Weight Decay: {optimizer_advanced.param_groups[0]['weight_decay']}\")\n",
        "print(f\"Label Smoothing: 0.1\")\n",
        "print(f\"Mixup Alpha: 1.0\")\n",
        "print(f\"Mixup Probability: 50%\")\n",
        "print(f\"Data Augmentation: RandomRotation + RandomAffine\")\n",
        "print(f\"Epochs: {epochs_advanced}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ ADVANCED TECHNIQUES - Training Loop\n",
        "\n",
        "print(\"Starting ADVANCED training with all techniques...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for epoch in range(1, epochs_advanced + 1):\n",
        "    # Advanced training with mixup and label smoothing\n",
        "    train_loss, train_acc = train_advanced(\n",
        "        model_advanced, device, train_loader, optimizer_advanced, epoch, \n",
        "        use_mixup=True, mixup_alpha=1.0\n",
        "    )\n",
        "    \n",
        "    # Advanced validation\n",
        "    val_loss, val_acc = validate_advanced(model_advanced, device, val_loader)\n",
        "    \n",
        "    # Advanced learning rate scheduling\n",
        "    scheduler_advanced.step(val_acc)\n",
        "    \n",
        "    # Store metrics\n",
        "    train_losses_advanced.append(train_loss)\n",
        "    train_accs_advanced.append(train_acc)\n",
        "    val_losses_advanced.append(val_loss)\n",
        "    val_accs_advanced.append(val_acc)\n",
        "    \n",
        "    # Print epoch results with current learning rate\n",
        "    current_lr = optimizer_advanced.param_groups[0]['lr']\n",
        "    print(f'Advanced Epoch {epoch:2d}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}% | LR: {current_lr:.6f}')\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc_advanced:\n",
        "        best_val_acc_advanced = val_acc\n",
        "        torch.save(model_advanced.state_dict(), 'best_advanced_model.pth')\n",
        "        print(f'  ‚Üí New best validation accuracy: {val_acc:.2f}%')\n",
        "    \n",
        "    # Early stopping if target achieved\n",
        "    if val_acc >= 99.4:\n",
        "        print(f'  ‚Üí Target accuracy of 99.4% achieved!')\n",
        "        break\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"ADVANCED training completed!\")\n",
        "print(f\"Best validation accuracy: {best_val_acc_advanced:.2f}%\")\n",
        "print(f\"Final learning rate: {optimizer_advanced.param_groups[0]['lr']:.6f}\")\n",
        "print(f\"Techniques used: Label Smoothing + Mixup + Data Augmentation + AdamW + ReduceLROnPlateau\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî¨ ADVANCED TECHNIQUES - Final Testing and Comparison\n",
        "\n",
        "# Load best advanced model and test\n",
        "print(\"Loading best ADVANCED model and testing on test set...\")\n",
        "model_advanced.load_state_dict(torch.load('best_advanced_model.pth'))\n",
        "\n",
        "# Test with standard loss function for fair comparison\n",
        "def test_standard(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = 100. * correct / len(test_loader.dataset)\n",
        "    \n",
        "    print(f'\\nTest Results:')\n",
        "    print(f'Average loss: {test_loss:.4f}')\n",
        "    print(f'Accuracy: {correct}/{len(test_loader.dataset)} ({test_acc:.2f}%)')\n",
        "    \n",
        "    return test_loss, test_acc\n",
        "\n",
        "test_loss_advanced, test_acc_advanced = test_standard(model_advanced, device, test_loader)\n",
        "\n",
        "# Plot advanced training curves\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses_advanced, label='Advanced Train Loss', color='blue', linewidth=2)\n",
        "plt.plot(val_losses_advanced, label='Advanced Val Loss', color='red', linewidth=2)\n",
        "plt.title('Advanced Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accs_advanced, label='Advanced Train Acc', color='blue', linewidth=2)\n",
        "plt.plot(val_accs_advanced, label='Advanced Val Acc', color='red', linewidth=2)\n",
        "plt.axhline(y=99.4, color='green', linestyle='--', linewidth=2, label='Target (99.4%)')\n",
        "plt.title('Advanced Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Comprehensive comparison\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ADVANCED TECHNIQUES RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Model Architecture: ImprovedNet with ALL Advanced Techniques\")\n",
        "print(f\"Total Parameters: {sum(p.numel() for p in model_advanced.parameters()):,}\")\n",
        "print(f\"Parameter Count < 20k: {sum(p.numel() for p in model_advanced.parameters()) < 20000}\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc_advanced:.2f}%\")\n",
        "print(f\"Final Test Accuracy: {test_acc_advanced:.2f}%\")\n",
        "print(f\"Target Achieved (‚â•99.4%): {'‚úÖ YES' if test_acc_advanced >= 99.4 else '‚ùå NO'}\")\n",
        "print(f\"Training Epochs Used: {len(train_losses_advanced)}\")\n",
        "print(f\"Epochs < 20: {'‚úÖ YES' if len(train_losses_advanced) <= 20 else '‚ùå NO'}\")\n",
        "print(f\"Final Learning Rate: {optimizer_advanced.param_groups[0]['lr']:.6f}\")\n",
        "print(\"=\"*80)\n",
        "print(\"ADVANCED TECHNIQUES USED:\")\n",
        "print(\"‚úÖ Label Smoothing (smoothing=0.1)\")\n",
        "print(\"‚úÖ Mixup Data Augmentation (Œ±=1.0, 50% probability)\")\n",
        "print(\"‚úÖ RandomRotation + RandomAffine\")\n",
        "print(\"‚úÖ AdamW Optimizer (lr=0.0008, weight_decay=1e-3)\")\n",
        "print(\"‚úÖ ReduceLROnPlateau (patience=2)\")\n",
        "print(\"‚úÖ Enhanced Architecture (more channels)\")\n",
        "print(\"‚úÖ Batch Normalization + Dropout\")\n",
        "print(\"‚úÖ Global Average Pooling\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üî¨ Advanced Techniques - Complete Implementation Summary\n",
        "\n",
        "## **üéØ Advanced Techniques Implemented:**\n",
        "\n",
        "### **1. üè∑Ô∏è Label Smoothing**\n",
        "- **Implementation**: Custom `LabelSmoothingCrossEntropy` class\n",
        "- **Smoothing Factor**: 0.1 (10% smoothing)\n",
        "- **Benefits**: Prevents overconfident predictions, improves generalization\n",
        "- **Formula**: `loss = (1-Œ±) * standard_loss + Œ± * uniform_loss`\n",
        "\n",
        "### **2. üé® Mixup Data Augmentation**\n",
        "- **Implementation**: `mixup_data()` and `mixup_criterion()` functions\n",
        "- **Alpha Parameter**: 1.0 (Beta distribution parameter)\n",
        "- **Probability**: 50% chance to apply mixup per batch\n",
        "- **Benefits**: Creates virtual training examples, reduces overfitting\n",
        "- **Formula**: `mixed_x = Œª * x_i + (1-Œª) * x_j`\n",
        "\n",
        "### **3. üîÑ Enhanced Training Functions**\n",
        "- **Advanced Training**: `train_advanced()` with mixup and label smoothing\n",
        "- **Advanced Validation**: `validate_advanced()` with label smoothing\n",
        "- **Smart Mixup**: 50% probability to apply mixup per batch\n",
        "- **Loss Combination**: Mixup + Label Smoothing for maximum benefit\n",
        "\n",
        "### **4. ‚öôÔ∏è Optimized Hyperparameters**\n",
        "- **Learning Rate**: 0.0008 (slightly reduced for stability)\n",
        "- **Weight Decay**: 1e-3 (stronger regularization)\n",
        "- **Scheduler Patience**: 2 epochs (faster adaptation)\n",
        "- **Mixup Alpha**: 1.0 (balanced mixing)\n",
        "\n",
        "## **üìä Expected Performance Improvements:**\n",
        "\n",
        "### **Cumulative Effect of All Techniques:**\n",
        "| Technique | Expected Improvement | Cumulative |\n",
        "|-----------|---------------------|------------|\n",
        "| Original Baseline | 98.36% | 98.36% |\n",
        "| Architecture Improvements | +0.3-0.5% | 98.7-98.9% |\n",
        "| Data Augmentation | +0.3-0.5% | 99.0-99.4% |\n",
        "| AdamW + Better LR | +0.2-0.3% | 99.2-99.7% |\n",
        "| Label Smoothing | +0.2-0.4% | 99.4-100.1% |\n",
        "| Mixup | +0.2-0.3% | 99.6-100.4% |\n",
        "\n",
        "### **Target Achievement Probability:**\n",
        "- **Conservative Estimate**: 99.4-99.6% (high probability of success)\n",
        "- **Optimistic Estimate**: 99.6-99.8% (excellent performance)\n",
        "- **Best Case**: 99.8%+ (outstanding results)\n",
        "\n",
        "## **üî¨ Technical Benefits:**\n",
        "\n",
        "### **Label Smoothing Benefits:**\n",
        "- **Prevents Overfitting**: Reduces overconfident predictions\n",
        "- **Better Calibration**: More realistic confidence scores\n",
        "- **Improved Generalization**: Works better on unseen data\n",
        "- **Stable Training**: Smoother loss landscape\n",
        "\n",
        "### **Mixup Benefits:**\n",
        "- **Virtual Examples**: Creates new training samples\n",
        "- **Better Boundaries**: Smoother decision boundaries\n",
        "- **Robustness**: More resistant to adversarial examples\n",
        "- **Regularization**: Implicit regularization effect\n",
        "\n",
        "### **Combined Effect:**\n",
        "- **Synergistic**: Label smoothing + Mixup work together\n",
        "- **Robust Training**: Multiple regularization techniques\n",
        "- **Better Convergence**: More stable training process\n",
        "- **Higher Accuracy**: Maximum performance potential\n",
        "\n",
        "## **üéØ Success Criteria:**\n",
        "- ‚úÖ **Architecture**: Enhanced with more channels\n",
        "- ‚úÖ **Data Augmentation**: RandomRotation + RandomAffine\n",
        "- ‚úÖ **Optimizer**: AdamW with better weight decay\n",
        "- ‚úÖ **Scheduling**: ReduceLROnPlateau with faster adaptation\n",
        "- ‚úÖ **Label Smoothing**: 0.1 smoothing factor\n",
        "- ‚úÖ **Mixup**: 50% probability, Œ±=1.0\n",
        "- ‚úÖ **All Requirements**: BN, Dropout, GAP, FC layer\n",
        "- üéØ **Target**: 99.4%+ accuracy with <20k parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimized CNN Architecture Explanation\n",
        "\n",
        "## Key Design Principles\n",
        "\n",
        "### 1. **Parameter Efficiency**\n",
        "- **Smaller channel progression**: 1‚Üí8‚Üí16‚Üí16‚Üí32‚Üí32 (vs original 1‚Üí32‚Üí64‚Üí128‚Üí256‚Üí512‚Üí1024)\n",
        "- **Global Average Pooling (GAP)**: Eliminates need for large fully connected layers\n",
        "- **Strategic pooling**: Only 2 max-pooling layers to preserve spatial information\n",
        "\n",
        "### 2. **Regularization Techniques**\n",
        "- **Batch Normalization**: After each conv layer for stable training\n",
        "- **Dropout2D**: 0.1 dropout in conv layers, 0.2 in final FC layer\n",
        "- **Weight Decay**: L2 regularization in optimizer (1e-4)\n",
        "\n",
        "### 3. **Training Optimizations**\n",
        "- **Adam Optimizer**: Better convergence than SGD for this architecture\n",
        "- **Learning Rate Scheduling**: StepLR with gamma=0.1 every 7 epochs\n",
        "- **Early Stopping**: Stops when 99.4% validation accuracy is reached\n",
        "\n",
        "### 4. **Architecture Details**\n",
        "```\n",
        "Input: 28√ó28√ó1\n",
        "‚îú‚îÄ‚îÄ Conv1: 1‚Üí8 channels, 3√ó3, padding=1 ‚Üí 28√ó28√ó8\n",
        "‚îú‚îÄ‚îÄ BN1 + ReLU + Dropout2D(0.1)\n",
        "‚îú‚îÄ‚îÄ Conv2: 8‚Üí16 channels, 3√ó3, padding=1 ‚Üí 28√ó28√ó16  \n",
        "‚îú‚îÄ‚îÄ BN2 + ReLU + Dropout2D(0.1)\n",
        "‚îú‚îÄ‚îÄ MaxPool2D(2√ó2) ‚Üí 14√ó14√ó16\n",
        "‚îú‚îÄ‚îÄ Conv3: 16‚Üí16 channels, 3√ó3, padding=1 ‚Üí 14√ó14√ó16\n",
        "‚îú‚îÄ‚îÄ BN3 + ReLU + Dropout2D(0.1)\n",
        "‚îú‚îÄ‚îÄ Conv4: 16‚Üí32 channels, 3√ó3, padding=1 ‚Üí 14√ó14√ó32\n",
        "‚îú‚îÄ‚îÄ BN4 + ReLU + Dropout2D(0.1)\n",
        "‚îú‚îÄ‚îÄ MaxPool2D(2√ó2) ‚Üí 7√ó7√ó32\n",
        "‚îú‚îÄ‚îÄ Conv5: 32‚Üí32 channels, 3√ó3, padding=1 ‚Üí 7√ó7√ó32\n",
        "‚îú‚îÄ‚îÄ BN5 + ReLU + Dropout2D(0.1)\n",
        "‚îú‚îÄ‚îÄ Global Average Pooling ‚Üí 1√ó1√ó32\n",
        "‚îú‚îÄ‚îÄ Dropout(0.2)\n",
        "‚îî‚îÄ‚îÄ FC: 32‚Üí10 ‚Üí 10 classes\n",
        "```\n",
        "\n",
        "### 5. **Parameter Count Breakdown**\n",
        "- Conv layers: ~7,000 parameters\n",
        "- BatchNorm layers: ~200 parameters  \n",
        "- FC layer: 330 parameters\n",
        "- **Total: ~7,500 parameters** (well under 20k limit)\n",
        "\n",
        "### 6. **Why This Works**\n",
        "- **GAP reduces overfitting** by eliminating spatial dependencies\n",
        "- **BatchNorm accelerates training** and provides regularization\n",
        "- **Progressive channel increase** captures features efficiently\n",
        "- **Strategic dropout** prevents overfitting without losing capacity\n",
        "- **Adam optimizer** with scheduling provides stable convergence\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
